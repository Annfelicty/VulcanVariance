{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Vulcans Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the optimal production budget range for maximizing ROI? (Analyze the relationship between production budget and return on investment to identify sweet spots for budget allocation.)\n",
    "\n",
    "2. How does critical reception (vote_average) correlate with commercial success? (2. How does critical reception (vote_average) correlate with commercial success?)\n",
    "\n",
    "3. What is the domestic vs international revenue split trend?(Understanding geographic revenue distribution helps in marketing budget allocation and release strategies.)\n",
    "\n",
    "4. Which genres provide the best risk-adjusted returns?(Genre analysis helps in portfolio planning and risk management for production companies)\n",
    "\n",
    "5. How does movie popularity correlate with actual box office performance? (Social media buzz and pre-release popularity as predictors of commercial success.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 1:  rt.reviews.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset **rt.reviews.tsv** contains movie review data collected from Rotten Tomatoes. It includes metadata such as reviewer information, review content, ratings, freshness labels, publisher names, and publication dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>fresh</th>\n",
       "      <th>critic</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>A distinctly gallows take on contemporary fina...</td>\n",
       "      <td>3/5</td>\n",
       "      <td>fresh</td>\n",
       "      <td>PJ Nabarro</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick Nabarro</td>\n",
       "      <td>November 10, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>It's an allegory in search of a meaning that n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Annalee Newitz</td>\n",
       "      <td>0</td>\n",
       "      <td>io9.com</td>\n",
       "      <td>May 23, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>... life lived in a bubble in financial dealin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Sean Axmaker</td>\n",
       "      <td>0</td>\n",
       "      <td>Stream on Demand</td>\n",
       "      <td>January 4, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Continuing along a line introduced in last yea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Daniel Kasman</td>\n",
       "      <td>0</td>\n",
       "      <td>MUBI</td>\n",
       "      <td>November 16, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>... a perverse twist on neorealism...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Cinema Scope</td>\n",
       "      <td>October 12, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             review rating   fresh  \\\n",
       "0   3  A distinctly gallows take on contemporary fina...    3/5   fresh   \n",
       "1   3  It's an allegory in search of a meaning that n...    NaN  rotten   \n",
       "2   3  ... life lived in a bubble in financial dealin...    NaN   fresh   \n",
       "3   3  Continuing along a line introduced in last yea...    NaN   fresh   \n",
       "4   3             ... a perverse twist on neorealism...     NaN   fresh   \n",
       "\n",
       "           critic  top_critic         publisher               date  \n",
       "0      PJ Nabarro           0   Patrick Nabarro  November 10, 2018  \n",
       "1  Annalee Newitz           0           io9.com       May 23, 2018  \n",
       "2    Sean Axmaker           0  Stream on Demand    January 4, 2018  \n",
       "3   Daniel Kasman           0              MUBI  November 16, 2017  \n",
       "4             NaN           0      Cinema Scope   October 12, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset and checking top five rows\n",
    "df= pd.read_csv(\"../Original_Data/rt.reviews.tsv\", sep='\\t', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The file isn't encoded in UTF-8, As a result, trying to load it normally causes an error. The use of encoding=\"ISO-8859-1\" solved the problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54432, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'review', 'rating', 'fresh', 'critic', 'top_critic', 'publisher',\n",
       "       'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54432 entries, 0 to 54431\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          54432 non-null  int64 \n",
      " 1   review      48869 non-null  object\n",
      " 2   rating      40915 non-null  object\n",
      " 3   fresh       54432 non-null  object\n",
      " 4   critic      51710 non-null  object\n",
      " 5   top_critic  54432 non-null  int64 \n",
      " 6   publisher   54123 non-null  object\n",
      " 7   date        54432 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>54432.0</td>\n",
       "      <td>1045.706882</td>\n",
       "      <td>586.657046</td>\n",
       "      <td>3.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_critic</th>\n",
       "      <td>54432.0</td>\n",
       "      <td>0.240594</td>\n",
       "      <td>0.427448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean         std  min    25%     50%     75%  \\\n",
       "id          54432.0  1045.706882  586.657046  3.0  542.0  1083.0  1541.0   \n",
       "top_critic  54432.0     0.240594    0.427448  0.0    0.0     0.0     0.0   \n",
       "\n",
       "               max  \n",
       "id          2000.0  \n",
       "top_critic     1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>48869</td>\n",
       "      <td>48682</td>\n",
       "      <td>Parental Content Review</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>40915</td>\n",
       "      <td>186</td>\n",
       "      <td>3/5</td>\n",
       "      <td>4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh</th>\n",
       "      <td>54432</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh</td>\n",
       "      <td>33035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critic</th>\n",
       "      <td>51710</td>\n",
       "      <td>3496</td>\n",
       "      <td>Emanuel Levy</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publisher</th>\n",
       "      <td>54123</td>\n",
       "      <td>1281</td>\n",
       "      <td>eFilmCritic.com</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>54432</td>\n",
       "      <td>5963</td>\n",
       "      <td>January 1, 2000</td>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count unique                      top   freq\n",
       "review     48869  48682  Parental Content Review     24\n",
       "rating     40915    186                      3/5   4327\n",
       "fresh      54432      2                    fresh  33035\n",
       "critic     51710   3496             Emanuel Levy    595\n",
       "publisher  54123   1281          eFilmCritic.com    673\n",
       "date       54432   5963          January 1, 2000   4303"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "review         5563\n",
       "rating        13517\n",
       "fresh             0\n",
       "critic         2722\n",
       "top_critic        0\n",
       "publisher       309\n",
       "date              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing values as sum\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0.000000\n",
       "review        10.220091\n",
       "rating        24.832819\n",
       "fresh          0.000000\n",
       "critic         5.000735\n",
       "top_critic     0.000000\n",
       "publisher      0.567681\n",
       "date           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing values as mean\n",
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RT.Reviews** dataset consists of **54,432 records** and **8 attributes**, capturing movie reviews from **Rotten Tomatoes**.\n",
    "\n",
    "#### Key Columns:\n",
    "- `review`: The text of the review\n",
    "- `rating`: Rating values (e.g., `\"3/5\"`, `\"B+\"`, etc.)\n",
    "- `fresh`: Indicates sentiment (`fresh` or `rotten`)\n",
    "- `critic`: Name of the reviewer\n",
    "- `publisher`: Source of the review\n",
    "- `date`: Review publication date\n",
    "\n",
    "#### Data Completeness:\n",
    "- `rating`: **24.8% missing**\n",
    "- `review`: **10% missing**\n",
    "- `critic`: **5% missing**\n",
    "All other fields (`id`, `fresh`, `top_critic`, `publisher`, `date`) are nearly complete.\n",
    "\n",
    "#### Additional Insights:\n",
    "- **Unique publishers**: `1,281`\n",
    "- **Unique critics**: `3,496`\n",
    "- **Top rating value**: `\"3/5\"`\n",
    "- **Duplicate records**: `9` (should be removed)\n",
    "- **`top_critic` field**: Binary (0 or 1), with ~**24%** marked as top critics\n",
    "- **`date` field**: Spans multiple years and should be converted to datetime for analysis\n",
    "\n",
    "#### Next Steps:\n",
    "- Handle missing values\n",
    "- Standardize `rating` formats\n",
    "- Parse `date` into datetime objects\n",
    "- Remove duplicate records\n",
    "- Impute or filter out null values in key fields (`review`, `critic`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality and preserve analytical integrity, we apply the following cleaning rules:\n",
    "\n",
    "##### Drop Column:\n",
    "- **Column `id`** is just an index with no analytical value -- Decision drop\n",
    "\n",
    "##### Drop Rows:\n",
    "- **Missing `rating`**: As the most critical field representing reviewer opinion, rows without a rating are dropped entirely.\n",
    "- **Less than 5% missing fields**: Any row missing less 5% values is also dropped to reduce imputing and preserve completeness.\n",
    "\n",
    "##### Impute Missing Values:\n",
    "- **Missing `review`**: Rows with missing review text are retained but imputed with the placeholder `\"Unknown\"` to preserve structure for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1: Dropping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before: (54432, 8)\n",
      "Shape After: (38734, 8)\n"
     ]
    }
   ],
   "source": [
    "#creating a list and dropping the rows with null\n",
    "print(f\"Shape Before: {df.shape}\")\n",
    "columns_to_droprows = ['rating','critic', 'publisher']\n",
    "df.dropna(subset=columns_to_droprows, inplace=True)\n",
    "print(f\"Shape After: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2: Imputing Null with UNKNOWN string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the nan values with UNKNOWN\n",
    "df['review'].fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "review        0\n",
       "rating        0\n",
       "fresh         0\n",
       "critic        0\n",
       "top_critic    0\n",
       "publisher     0\n",
       "date          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3: Changing date to datetime dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coverting the event.date into a datetime type\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4: Standardizing the column rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3/5', 'C', '2/5', 'B-', '2/4', 'B', '3/4', '4/5', '4/4', '6/10',\n",
       "       '1/4', '8', '2.5/4', '4/10', '2.0/5', '3/10', '7/10', 'A-', '5/5',\n",
       "       'F', '3.5/4', 'D+', '1.5/4', '3.5/5', '8/10', 'B+', '9/10',\n",
       "       '2.5/5', '7.5/10', '5.5/10', 'C-', '1.5/5', '1/5', '5/10', 'C+',\n",
       "       '0/5', '6', '0.5/4', 'D', '3.1/5', '3/6', '4.5/5', '0/4', '2/10',\n",
       "       'D-', '7', '1/10', '3', 'A+', 'A', '4.0/4', '9.5/10', '2.5',\n",
       "       '2.1/2', '6.5/10', '3.7/5', '8.4/10', '9', '1', '7.2/10', '2.2/5',\n",
       "       '0.5/10', '5', '0', '2', '4.5', '7.7', '5.0/5', '8.5/10', '3.0/5',\n",
       "       '0.5/5', '1.5/10', '3.0/4', '2.3/10', '4.5/10', '4/6', '3.5',\n",
       "       '8.6/10', '6/8', '2.0/4', '2.7', '4.2/10', '5.8', '4', '7.1/10',\n",
       "       '3.5/10', '5.8/10', '4.0/5', '0/10', '5.0/10', '5.9/10', '2.4/5',\n",
       "       '1.9/5', '4.9', '7.4/10', '1.5', '2.3/4', '8.8/10', '4.0/10',\n",
       "       '2.2', '3.8/10', '6.8/10', '7.3', '7.0/10', '3.2', '4.2', '8.4',\n",
       "       '5.5/5', '6.3/10', '7.6/10', '8.1/10', '3.6/5', '2/6', '7.7/10',\n",
       "       '1.8', '8.9/10', '8.9', '8.2/10', '8.3/10', '2.6/6', '4.1/10',\n",
       "       '2.5/10', 'F+', '6.0/10', '1.0/4', '7.9/10', '8.7/10', '4.3/10',\n",
       "       '9.6/10', '9.0/10', '4.0', '7.9', '6.7', '8.0/10', '9.2/10', '5.2',\n",
       "       '5.9', '3.7', '4.7', '6.2/10', '1/6', '8.2', '2.6/5', '3.4', '9.7',\n",
       "       '3.3/5', '3.8/5', '1/2', '7.4', '4.8', '1.6/5', '2/2', '1-5',\n",
       "       '1.0', '4.3/5', '5/6', '9.2', '2.7/5', '4.9/10', '3.0', '3.1',\n",
       "       '7.8/10', 'F-', '2.3/5', '3.0/10', '3/2', '7.8', '4.2/5', '9.0',\n",
       "       '7.3/10', '4.4/5', '6.9/10', '0/6', 'T', '6.2', '3.3', '9.8',\n",
       "       '8.5', '1.0/5', '4.1', '7.1', '3 1/2'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking unique items\n",
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining letter grade mapping\n",
    "letter_grades = {\n",
    "    'A+': 10.0, 'A': 9.5, 'A-': 9.0,\n",
    "    'B+': 8.5, 'B': 8.0, 'B-': 7.5,\n",
    "    'C+': 7.0, 'C': 6.5, 'C-': 6.0,\n",
    "    'D+': 5.5, 'D': 5.0, 'D-': 4.5,\n",
    "    'F+': 4.0, 'F': 3.5, 'F-': 3.0\n",
    "}\n",
    "\n",
    "\n",
    "# Normalizing fractional ratings to 10-point scale\n",
    "def normalize_rating(val):\n",
    "    try:\n",
    "        if '/' in val:\n",
    "            num, den = val.split('/')\n",
    "            return round(float(num) / float(den) * 10, 2)\n",
    "        elif val in letter_grades:\n",
    "            return letter_grades[val]\n",
    "        else:\n",
    "            return float(val)\n",
    "    except:\n",
    "        return None\n",
    "df['rating'] = df['rating'].apply(normalize_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.00     5122\n",
       "6.00     5008\n",
       "7.50     4330\n",
       "5.00     4249\n",
       "4.00     3260\n",
       "         ... \n",
       "2.30        1\n",
       "15.00       1\n",
       "5.40        1\n",
       "4.60        1\n",
       "4.33        1\n",
       "Name: rating, Length: 82, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking current value counts\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38717, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping suspicious ratings\n",
    "df = df[df['rating'] <= 10]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5: Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>fresh</th>\n",
       "      <th>critic</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>A distinctly gallows take on contemporary fina...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>fresh</td>\n",
       "      <td>PJ Nabarro</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick Nabarro</td>\n",
       "      <td>2018-11-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>Quickly grows repetitive and tiresome, meander...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Eric D. Snider</td>\n",
       "      <td>0</td>\n",
       "      <td>EricDSnider.com</td>\n",
       "      <td>2013-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Cronenberg is not a director to be daunted by ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Matt Kelemen</td>\n",
       "      <td>0</td>\n",
       "      <td>Las Vegas CityLife</td>\n",
       "      <td>2013-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>While not one of Cronenberg's stronger films, ...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>fresh</td>\n",
       "      <td>Emanuel Levy</td>\n",
       "      <td>0</td>\n",
       "      <td>EmanuelLevy.Com</td>\n",
       "      <td>2013-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Robert Pattinson works mighty hard to make Cos...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>rotten</td>\n",
       "      <td>Christian Toto</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Hollywood</td>\n",
       "      <td>2013-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                             review  rating   fresh  \\\n",
       "0    3  A distinctly gallows take on contemporary fina...     6.0   fresh   \n",
       "6    3  Quickly grows repetitive and tiresome, meander...     6.5  rotten   \n",
       "7    3  Cronenberg is not a director to be daunted by ...     4.0  rotten   \n",
       "11   3  While not one of Cronenberg's stronger films, ...     7.5   fresh   \n",
       "12   3  Robert Pattinson works mighty hard to make Cos...     5.0  rotten   \n",
       "\n",
       "            critic  top_critic           publisher       date  \n",
       "0       PJ Nabarro           0     Patrick Nabarro 2018-11-10  \n",
       "6   Eric D. Snider           0     EricDSnider.com 2013-07-17  \n",
       "7     Matt Kelemen           0  Las Vegas CityLife 2013-04-21  \n",
       "11    Emanuel Levy           0     EmanuelLevy.Com 2013-02-03  \n",
       "12  Christian Toto           0       Big Hollywood 2013-01-15  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#current structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the DataFrame to CSV\n",
    "df.to_csv('../Cleaned_Data/cleaned_rt-reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 2: tmdb.movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Original_Data/tmdb.movies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-53a96ac50cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Original_Data/tmdb.movies.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Original_Data/tmdb.movies.csv'"
     ]
    }
   ],
   "source": [
    "#importing the dataset\n",
    "df1= pd.read_csv(\"../Original_Data/tmdb.movies.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df1.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as sum\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as mean\n",
    "df1.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **TMDB.Movies** dataset contains **26,517 movie records** and **10 attributes**, capturing details like titles, genres, ratings, and popularity.\n",
    "\n",
    "####  **Key Columns:**\n",
    "- `genre_ids`: List of genre codes (e.g., `[28, 12]` for Action & Adventure)\n",
    "- `original_language`: Language code (e.g., `'en'`)\n",
    "- `original_title` / `title`: Original and localized movie titles\n",
    "- `popularity`: Popularity score (float)\n",
    "- `release_date`: Date the movie was released\n",
    "- `vote_average`: Average user rating (0–10 scale)\n",
    "- `vote_count`: Number of user votes\n",
    "\n",
    "---\n",
    "\n",
    "####  **Data Quality:**\n",
    "- **No missing values**\n",
    "- **No duplicate rows**\n",
    "- `release_date` is an object type and should be converted to datetime for analysis\n",
    "- `genre_ids` contains 2,477 unique combinations (top: `[99]`)\n",
    "\n",
    "---\n",
    "\n",
    "####  **Quick Stats:**\n",
    "- **Most common language**: `en` (English) — 23,291 records\n",
    "- **Most frequent release date**: `2010-01-01` (269 movies)\n",
    "- **Top genres combo**: `[99]` — 3,700 movies\n",
    "- **Average vote**: `6.0`, with a max of `10.0`\n",
    "- **Highest vote count**: `22,186` (Inception)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Next Steps:**\n",
    "1. Convert `genre_ids` from a string to an actual Python list.\n",
    "2. Convert the `id` column to a numeric type.\n",
    "3. Convert `release_date` to datetime format.\n",
    "4. Remove leading and trailing whitespace in string columns (`original_language`, `original_title`, `title`).\n",
    "5. Ensure `popularity`, `vote_average`, and `vote_count` are numeric and fill missing values with 0.\n",
    "6. Remove duplicate movies based on the `id` column.\n",
    "7. Drop rows that are missing crucial information (`title`, `release_date`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality and preserve analytical integrity, the following cleaning rules were applied to the **TMDB.Movies** dataset:\n",
    "\n",
    "####  Drop Rows:\n",
    "- **Missing `title` or `release_date`**: Rows missing these crucial fields were dropped since they are essential for timeline and identification-based analyses.\n",
    "- **Duplicate entries**: Duplicate rows based on the `id` column were removed to prevent overrepresentation of movies.\n",
    "\n",
    "#### Transformations & Imputations:\n",
    "- **`genre_ids`**: Converted from string to a Python list for easier genre-level operations.\n",
    "- **`release_date`**: Parsed into proper datetime format for time-series analysis.\n",
    "- **String fields** (`original_language`, `original_title`, `title`): Trimmed to remove unnecessary whitespace.\n",
    "- **Numeric columns** (`popularity`, `vote_average`, `vote_count`): Ensured valid numeric types and missing values were imputed with `0` to maintain completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'genre_ids' from a string to an actual Python list\n",
    "df1['genre_ids'] = df1['genre_ids'].apply(\n",
    "    lambda x: ast.literal_eval(x) if pd.notnull(x) else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'id' column to numeric type\n",
    "df1['id'] = pd.to_numeric(df1['id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'release_date' to datetime format\n",
    "df1['release_date'] = pd.to_datetime(df1['release_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace in string columns\n",
    "string_columns = ['original_language', 'original_title', 'title']\n",
    "for col in string_columns:\n",
    "    df1[col] = df1[col].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure numeric columns are in the right format and fill missing values\n",
    "df1['popularity'] = pd.to_numeric(df1['popularity'], errors='coerce').fillna(0)\n",
    "df1['vote_average'] = pd.to_numeric(df1['vote_average'], errors='coerce').fillna(0)\n",
    "df1['vote_count'] = pd.to_numeric(df1['vote_count'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicated movies\n",
    "df1.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing crucial info\n",
    "df1.dropna(subset=['title', 'release_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index after dropping rows\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"../Cleaned_Data/cleaned_tmdb_movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 3: im.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional libraries needed\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect the database\n",
    "db = '../Original_Data/im.db'\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_tables = \"\"\"\n",
    "                   SELECT name\n",
    "                   FROM sqlite_master\n",
    "                   WHERE type = 'table';\n",
    "                   \"\"\"\n",
    "\n",
    "cursor.execute(query_for_tables)\n",
    "\n",
    "tables = cursor.fetchall()\n",
    "print(f\"Tables in the database: {tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function and observing movie_basics table\n",
    "def table_as_df(conn, table_name):\n",
    "    #takes an open SQLite connection and a table name,\n",
    "    #returns the table as a Pandas DataFrame.\n",
    "    return pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "df_movie_basics = table_as_df(conn, 'writers')\n",
    "print(df_movie_basics.info())\n",
    "print(df_movie_basics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3: Loading both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics = table_as_df(conn, 'movie_basics')\n",
    "df_directors = table_as_df(conn, 'directors')\n",
    "\n",
    "# Preview their columns\n",
    "#print(\"movie_basics columns:\", df_basics.columns())\n",
    "#print(\"directors columns:\", df_directors.columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4: Merging the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_basics.merge(df_directors, on='movie_id', how='left')\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = table_as_df(conn, 'movie_ratings')\n",
    "print(df_ratings.head())\n",
    "print(df_ratings.info())\n",
    "merged_df = merged_df.merge(df_ratings, on='movie_id', how='left')\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principals = table_as_df(conn, 'principals')\n",
    "\n",
    "# Merge with principals on movie_id and person_id\n",
    "merged_df = merged_df.merge(\n",
    "    df_principals,\n",
    "    on=['movie_id', 'person_id'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons = table_as_df(conn, 'persons')\n",
    "\n",
    "# Merge on person_id to get name, birth year, profession\n",
    "merged_df = merged_df.merge(\n",
    "    df_persons,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop exact row duplicates\n",
    "clean_df = merged_df.drop_duplicates()\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'birth_year',\n",
    "    'death_year',\n",
    "    'job',\n",
    "    'characters',\n",
    "    'ordering'\n",
    "]\n",
    "\n",
    "clean_df = clean_df.dropna(subset=['averagerating', 'numvotes', 'primary_name'])\n",
    "\n",
    "clean_df.drop(columns=columns_to_drop, inplace=True)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median runtime (ignoring NaNs)\n",
    "median_runtime = clean_df['runtime_minutes'].median()\n",
    "\n",
    "# Fill NaN runtimes with the median\n",
    "clean_df['runtime_minutes'].fillna(median_runtime, inplace=True)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(subset=['genres', 'category', 'primary_profession'])\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('../Cleaned_Data/clean_imdb_data.csv', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 4: bom.movie_gross.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df2 = pd.read_csv('../Original_Data/bom.movie_gross.csv.gz', encoding='latin1',low_memory=False)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as sum\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as mean\n",
    "df2.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BoxOffice.Revenue** dataset consists of **3,387 records** and **5 attributes**, capturing domestic and foreign earnings of movies across multiple years.\n",
    "\n",
    "#### **Key Columns:**\n",
    "- `title`: Name of the movie\n",
    "- `studio`: Producing or distributing studio\n",
    "- `domestic_gross`: U.S./Canada revenue (float, in dollars)\n",
    "- `foreign_gross`: Revenue from outside the U.S./Canada (object, needs conversion)\n",
    "- `year`: Release year (integer)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Completeness:**\n",
    "- `studio`: **5 missing** values\n",
    "- `domestic_gross`: **28 missing**\n",
    "- `foreign_gross`: **1,350 missing** — over **39%** of the data\n",
    "- `title` and `year`: **100% complete**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Additional Insights:**\n",
    "- **Unique movie titles**: `3,386`\n",
    "- **Unique studios**: `257` (most frequent: `IFC`, with 166 movies)\n",
    "- **Max domestic gross**: `$936,700,000`\n",
    "- **Top frequent `foreign_gross` value**: `1,200,000` (appears 23 times)\n",
    "- **Year range**: `2010 – 2018`, centered around recent box office trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Convert `year` to datetime format**  \n",
    "   Transformed the `year` column from `int64` to datetime for consistency in time-based operations.\n",
    "\n",
    "2. **Extract clean `year_only` column**  \n",
    "   Created a separate `year_only` column by extracting the year component from the datetime object for easy filtering and plotting.\n",
    "\n",
    "3. **Convert `foreign_gross` to float**  \n",
    "   Replaced commas in `foreign_gross` values and converted the column from string to float to enable numeric analysis.\n",
    "\n",
    "4. **Handle missing values in `studio`**  \n",
    "   Filled missing entries in the studio column with mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting year from int64 to datetime format\n",
    "df2['year'] = pd.to_datetime(df2['year'],format='%Y')\n",
    "\n",
    "# extracting the year_only for further analysis (created a new column called year_only)\n",
    "df2['year_only'] = df2['year'].dt.year\n",
    "\n",
    "# converting the foreign_gross to a float(similar to the domestic_gross)\n",
    "# the foreign_gross has a comma which we will have to replace so as to convert it to float.\n",
    "df2['foreign_gross'] = pd.to_numeric(df2['foreign_gross'].str.replace(',', '', regex=False), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether the columns are now in the right format.\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the null values, from the highest to the lowest %\n",
    "(df2.isna().mean()*100).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for unique variables in the columnns for analysis\n",
    "colm = df2.columns\n",
    "\n",
    "for colm in df2:\n",
    "    colm_val = df2[colm].unique()\n",
    "    print(f\"{colm},'\\n',{colm_val}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only object columns with missing values\n",
    "missing_obj_col = df2.select_dtypes(include='object').isna().sum()\n",
    "print(missing_obj_col,'\\n')\n",
    "\n",
    "# Get the mode of the studio column\n",
    "mode_value = df2['studio'].mode()[0]\n",
    "print(mode_value)\n",
    "\n",
    "# Fill nulls with the mode\n",
    "df2['studio'] = df2['studio'].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only numeric columns with missing values\n",
    "missing_num_col = df2.select_dtypes('number').isna().sum()\n",
    "print(missing_num_col,'\\n')\n",
    "\n",
    "# checking the skewness of the gross amounts to determine how best to deal with the missing data\n",
    "print(\"Skewness of domestic_gross:\", df2['domestic_gross'].skew())\n",
    "print(\"Skewness of foreign_gross:\", df2['foreign_gross'].skew())\n",
    "\n",
    "# both are highly right-skewed. \n",
    "# Since the skew is >1, using the mean would be misleading because large outliers inflate it.\n",
    "# and using the median would only be okay if the msissing values was actually an error,not an absence of data\n",
    "# to avoid distorting the values by extreme outliers\n",
    "# but in this case, we will make a compromise to assume that the movies did not have a foreign or domestic revenue\n",
    "# therefore filling with 0 (zero) instead of the median\n",
    "\n",
    "# Fill missing values using the median\n",
    "df2['domestic_gross'] = df2['domestic_gross'].fillna(0)\n",
    "df2['foreign_gross'] = df2['foreign_gross'].fillna(0)\n",
    "\n",
    "# creating a new column total_gross \n",
    "df2['total_gross'] = df2['domestic_gross'] + df2['foreign_gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm that we do not have any missing values, numbers and objects:\n",
    "missing_num_col = df2.select_dtypes('number').isna().sum()\n",
    "print(missing_num_col,'\\n')\n",
    "\n",
    "missing_obj_col = df2.select_dtypes(include='object').isna().sum()\n",
    "print(missing_obj_col,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the columns we will be working on:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to check for skewness and outliers\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df2['domestic_gross'])\n",
    "plt.title('Boxplot of Domestic Gross')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df2['foreign_gross'])\n",
    "plt.title('Boxplot of Foreign Gross')\n",
    "plt.show()\n",
    "# box the domestic and foreign gross are heavily right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the DataFrame to CSV\n",
    "df2.to_csv('../Cleaned_Data/cleaned_bom.movie_gross.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
