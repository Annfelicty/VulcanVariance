{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Vulcans Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the optimal production budget range for maximizing ROI? (Analyze the relationship between production budget and return on investment to identify sweet spots for budget allocation.)\n",
    "\n",
    "2. How does critical reception (vote_average) correlate with commercial success? (2. How does critical reception (vote_average) correlate with commercial success?)\n",
    "\n",
    "3. What is the domestic vs international revenue split trend?(Understanding geographic revenue distribution helps in marketing budget allocation and release strategies.)\n",
    "\n",
    "4. Which genres provide the best risk-adjusted returns?(Genre analysis helps in portfolio planning and risk management for production companies)\n",
    "\n",
    "5. How does movie popularity correlate with actual box office performance? (Social media buzz and pre-release popularity as predictors of commercial success.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 1:  rt.reviews.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset **rt.reviews.tsv** contains movie review data collected from Rotten Tomatoes. It includes metadata such as reviewer information, review content, ratings, freshness labels, publisher names, and publication dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Original_Data/rt.reviews.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Reading the dataset and checking top five rows\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Original_Data/rt.reviews.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Original_Data/rt.reviews.tsv'"
     ]
    }
   ],
   "source": [
    "#Reading the dataset and checking top five rows\n",
    "df= pd.read_csv(\"../Original_Data/rt.reviews.tsv\", sep='\\t', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The file isn't encoded in UTF-8, As a result, trying to load it normally causes an error. The use of encoding=\"ISO-8859-1\" solved the problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#checking shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#checking shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as sum\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as mean\n",
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RT.Reviews** dataset consists of **54,432 records** and **8 attributes**, capturing movie reviews from **Rotten Tomatoes**.\n",
    "\n",
    "#### Key Columns:\n",
    "- `review`: The text of the review\n",
    "- `rating`: Rating values (e.g., `\"3/5\"`, `\"B+\"`, etc.)\n",
    "- `fresh`: Indicates sentiment (`fresh` or `rotten`)\n",
    "- `critic`: Name of the reviewer\n",
    "- `publisher`: Source of the review\n",
    "- `date`: Review publication date\n",
    "\n",
    "#### Data Completeness:\n",
    "- `rating`: **24.8% missing**\n",
    "- `review`: **10% missing**\n",
    "- `critic`: **5% missing**\n",
    "All other fields (`id`, `fresh`, `top_critic`, `publisher`, `date`) are nearly complete.\n",
    "\n",
    "#### Additional Insights:\n",
    "- **Unique publishers**: `1,281`\n",
    "- **Unique critics**: `3,496`\n",
    "- **Top rating value**: `\"3/5\"`\n",
    "- **Duplicate records**: `9` (should be removed)\n",
    "- **`top_critic` field**: Binary (0 or 1), with ~**24%** marked as top critics\n",
    "- **`date` field**: Spans multiple years and should be converted to datetime for analysis\n",
    "\n",
    "#### Next Steps:\n",
    "- Handle missing values\n",
    "- Standardize `rating` formats\n",
    "- Parse `date` into datetime objects\n",
    "- Remove duplicate records\n",
    "- Impute or filter out null values in key fields (`review`, `critic`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality and preserve analytical integrity, we apply the following cleaning rules:\n",
    "\n",
    "##### Drop Column:\n",
    "- **Column `id`** is just an index with no analytical value -- Decision drop\n",
    "\n",
    "##### Drop Rows:\n",
    "- **Missing `rating`**: As the most critical field representing reviewer opinion, rows without a rating are dropped entirely.\n",
    "- **Less than 5% missing fields**: Any row missing less 5% values is also dropped to reduce imputing and preserve completeness.\n",
    "\n",
    "##### Impute Missing Values:\n",
    "- **Missing `review`**: Rows with missing review text are retained but imputed with the placeholder `\"Unknown\"` to preserve structure for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1: Dropping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list and dropping the rows with null\n",
    "print(f\"Shape Before: {df.shape}\")\n",
    "columns_to_droprows = ['rating','critic', 'publisher']\n",
    "df.dropna(subset=columns_to_droprows, inplace=True)\n",
    "print(f\"Shape After: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2: Imputing Null with UNKNOWN string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the nan values with UNKNOWN\n",
    "df['review'].fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3: Changing date to datetime dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverting the event.date into a datetime type\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4: Standardizing the column rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking unique items\n",
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining letter grade mapping\n",
    "letter_grades = {\n",
    "    'A+': 10.0, 'A': 9.5, 'A-': 9.0,\n",
    "    'B+': 8.5, 'B': 8.0, 'B-': 7.5,\n",
    "    'C+': 7.0, 'C': 6.5, 'C-': 6.0,\n",
    "    'D+': 5.5, 'D': 5.0, 'D-': 4.5,\n",
    "    'F+': 4.0, 'F': 3.5, 'F-': 3.0\n",
    "}\n",
    "\n",
    "\n",
    "# Normalizing fractional ratings to 10-point scale\n",
    "def normalize_rating(val):\n",
    "    try:\n",
    "        if '/' in val:\n",
    "            num, den = val.split('/')\n",
    "            return round(float(num) / float(den) * 10, 2)\n",
    "        elif val in letter_grades:\n",
    "            return letter_grades[val]\n",
    "        else:\n",
    "            return float(val)\n",
    "    except:\n",
    "        return None\n",
    "df['rating'] = df['rating'].apply(normalize_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking current value counts\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping suspicious ratings\n",
    "df = df[df['rating'] <= 10]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5: Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the DataFrame to CSV\n",
    "df.to_csv('../Cleaned_Data/cleaned_rt-reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 2: tmdb.movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "df1= pd.read_csv(\"../Original_Data/tmdb.movies.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df1.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as sum\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as mean\n",
    "df1.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **TMDB.Movies** dataset contains **26,517 movie records** and **10 attributes**, capturing details like titles, genres, ratings, and popularity.\n",
    "\n",
    "####  **Key Columns:**\n",
    "- `genre_ids`: List of genre codes (e.g., `[28, 12]` for Action & Adventure)\n",
    "- `original_language`: Language code (e.g., `'en'`)\n",
    "- `original_title` / `title`: Original and localized movie titles\n",
    "- `popularity`: Popularity score (float)\n",
    "- `release_date`: Date the movie was released\n",
    "- `vote_average`: Average user rating (0–10 scale)\n",
    "- `vote_count`: Number of user votes\n",
    "\n",
    "---\n",
    "\n",
    "####  **Data Quality:**\n",
    "- **No missing values**\n",
    "- **No duplicate rows**\n",
    "- `release_date` is an object type and should be converted to datetime for analysis\n",
    "- `genre_ids` contains 2,477 unique combinations (top: `[99]`)\n",
    "\n",
    "---\n",
    "\n",
    "####  **Quick Stats:**\n",
    "- **Most common language**: `en` (English) — 23,291 records\n",
    "- **Most frequent release date**: `2010-01-01` (269 movies)\n",
    "- **Top genres combo**: `[99]` — 3,700 movies\n",
    "- **Average vote**: `6.0`, with a max of `10.0`\n",
    "- **Highest vote count**: `22,186` (Inception)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Next Steps:**\n",
    "1. Convert `genre_ids` from a string to an actual Python list.\n",
    "2. Convert the `id` column to a numeric type.\n",
    "3. Convert `release_date` to datetime format.\n",
    "4. Remove leading and trailing whitespace in string columns (`original_language`, `original_title`, `title`).\n",
    "5. Ensure `popularity`, `vote_average`, and `vote_count` are numeric and fill missing values with 0.\n",
    "6. Remove duplicate movies based on the `id` column.\n",
    "7. Drop rows that are missing crucial information (`title`, `release_date`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data quality and preserve analytical integrity, the following cleaning rules were applied to the **TMDB.Movies** dataset:\n",
    "\n",
    "####  Drop Rows:\n",
    "- **Missing `title` or `release_date`**: Rows missing these crucial fields were dropped since they are essential for timeline and identification-based analyses.\n",
    "- **Duplicate entries**: Duplicate rows based on the `id` column were removed to prevent overrepresentation of movies.\n",
    "\n",
    "#### Transformations & Imputations:\n",
    "- **`genre_ids`**: Converted from string to a Python list for easier genre-level operations.\n",
    "- **`release_date`**: Parsed into proper datetime format for time-series analysis.\n",
    "- **String fields** (`original_language`, `original_title`, `title`): Trimmed to remove unnecessary whitespace.\n",
    "- **Numeric columns** (`popularity`, `vote_average`, `vote_count`): Ensured valid numeric types and missing values were imputed with `0` to maintain completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'genre_ids' from a string to an actual Python list\n",
    "df1['genre_ids'] = df1['genre_ids'].apply(\n",
    "    lambda x: ast.literal_eval(x) if pd.notnull(x) else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'id' column to numeric type\n",
    "df1['id'] = pd.to_numeric(df1['id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'release_date' to datetime format\n",
    "df1['release_date'] = pd.to_datetime(df1['release_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace in string columns\n",
    "string_columns = ['original_language', 'original_title', 'title']\n",
    "for col in string_columns:\n",
    "    df1[col] = df1[col].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure numeric columns are in the right format and fill missing values\n",
    "df1['popularity'] = pd.to_numeric(df1['popularity'], errors='coerce').fillna(0)\n",
    "df1['vote_average'] = pd.to_numeric(df1['vote_average'], errors='coerce').fillna(0)\n",
    "df1['vote_count'] = pd.to_numeric(df1['vote_count'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicated movies\n",
    "df1.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing crucial info\n",
    "df1.dropna(subset=['title', 'release_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index after dropping rows\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"../Cleaned_Data/cleaned_tmdb_movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 3: im.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional libraries needed\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect the database\n",
    "db = '../Original_Data/im.db'\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_tables = \"\"\"\n",
    "                   SELECT name\n",
    "                   FROM sqlite_master\n",
    "                   WHERE type = 'table';\n",
    "                   \"\"\"\n",
    "\n",
    "cursor.execute(query_for_tables)\n",
    "\n",
    "tables = cursor.fetchall()\n",
    "print(f\"Tables in the database: {tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function and observing movie_basics table\n",
    "def table_as_df(conn, table_name):\n",
    "    #takes an open SQLite connection and a table name,\n",
    "    #returns the table as a Pandas DataFrame.\n",
    "    return pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "df_movie_basics = table_as_df(conn, 'writers')\n",
    "print(df_movie_basics.info())\n",
    "print(df_movie_basics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3: Loading both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics = table_as_df(conn, 'movie_basics')\n",
    "df_directors = table_as_df(conn, 'directors')\n",
    "\n",
    "# Preview their columns\n",
    "#print(\"movie_basics columns:\", df_basics.columns())\n",
    "#print(\"directors columns:\", df_directors.columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4: Merging the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_basics.merge(df_directors, on='movie_id', how='left')\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = table_as_df(conn, 'movie_ratings')\n",
    "print(df_ratings.head())\n",
    "print(df_ratings.info())\n",
    "merged_df = merged_df.merge(df_ratings, on='movie_id', how='left')\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principals = table_as_df(conn, 'principals')\n",
    "\n",
    "# Merge with principals on movie_id and person_id\n",
    "merged_df = merged_df.merge(\n",
    "    df_principals,\n",
    "    on=['movie_id', 'person_id'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons = table_as_df(conn, 'persons')\n",
    "\n",
    "# Merge on person_id to get name, birth year, profession\n",
    "merged_df = merged_df.merge(\n",
    "    df_persons,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop exact row duplicates\n",
    "clean_df = merged_df.drop_duplicates()\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'birth_year',\n",
    "    'death_year',\n",
    "    'job',\n",
    "    'characters',\n",
    "    'ordering'\n",
    "]\n",
    "\n",
    "clean_df = clean_df.dropna(subset=['averagerating', 'numvotes', 'primary_name'])\n",
    "\n",
    "clean_df.drop(columns=columns_to_drop, inplace=True)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median runtime (ignoring NaNs)\n",
    "median_runtime = clean_df['runtime_minutes'].median()\n",
    "\n",
    "# Fill NaN runtimes with the median\n",
    "clean_df['runtime_minutes'].fillna(median_runtime, inplace=True)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(subset=['genres', 'category', 'primary_profession'])\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('../Cleaned_Data/clean_imdb_data.csv', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 4: bom.movie_gross.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df2 = pd.read_csv('../Original_Data/bom.movie_gross.csv.gz', encoding='latin1',low_memory=False)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2: Basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking columns\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3: Overview of column types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4: Summary statistics numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5: Summary statistics categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as sum\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values as mean\n",
    "df2.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7: Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BoxOffice.Revenue** dataset consists of **3,387 records** and **5 attributes**, capturing domestic and foreign earnings of movies across multiple years.\n",
    "\n",
    "#### **Key Columns:**\n",
    "- `title`: Name of the movie\n",
    "- `studio`: Producing or distributing studio\n",
    "- `domestic_gross`: U.S./Canada revenue (float, in dollars)\n",
    "- `foreign_gross`: Revenue from outside the U.S./Canada (object, needs conversion)\n",
    "- `year`: Release year (integer)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Completeness:**\n",
    "- `studio`: **5 missing** values\n",
    "- `domestic_gross`: **28 missing**\n",
    "- `foreign_gross`: **1,350 missing** — over **39%** of the data\n",
    "- `title` and `year`: **100% complete**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Additional Insights:**\n",
    "- **Unique movie titles**: `3,386`\n",
    "- **Unique studios**: `257` (most frequent: `IFC`, with 166 movies)\n",
    "- **Max domestic gross**: `$936,700,000`\n",
    "- **Top frequent `foreign_gross` value**: `1,200,000` (appears 23 times)\n",
    "- **Year range**: `2010 – 2018`, centered around recent box office trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Data Cleaning Strategy Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Convert `year` to datetime format**  \n",
    "   Transformed the `year` column from `int64` to datetime for consistency in time-based operations.\n",
    "\n",
    "2. **Extract clean `year_only` column**  \n",
    "   Created a separate `year_only` column by extracting the year component from the datetime object for easy filtering and plotting.\n",
    "\n",
    "3. **Convert `foreign_gross` to float**  \n",
    "   Replaced commas in `foreign_gross` values and converted the column from string to float to enable numeric analysis.\n",
    "\n",
    "4. **Handle missing values in `studio`**  \n",
    "   Filled missing entries in the studio column with mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting year from int64 to datetime format\n",
    "df2['year'] = pd.to_datetime(df2['year'],format='%Y')\n",
    "\n",
    "# extracting the year_only for further analysis (created a new column called year_only)\n",
    "df2['year_only'] = df2['year'].dt.year\n",
    "\n",
    "# converting the foreign_gross to a float(similar to the domestic_gross)\n",
    "# the foreign_gross has a comma which we will have to replace so as to convert it to float.\n",
    "df2['foreign_gross'] = pd.to_numeric(df2['foreign_gross'].str.replace(',', '', regex=False), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether the columns are now in the right format.\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the null values, from the highest to the lowest %\n",
    "(df2.isna().mean()*100).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for unique variables in the columnns for analysis\n",
    "colm = df2.columns\n",
    "\n",
    "for colm in df2:\n",
    "    colm_val = df2[colm].unique()\n",
    "    print(f\"{colm},'\\n',{colm_val}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only object columns with missing values\n",
    "missing_obj_col = df2.select_dtypes(include='object').isna().sum()\n",
    "print(missing_obj_col,'\\n')\n",
    "\n",
    "# Get the mode of the studio column\n",
    "mode_value = df2['studio'].mode()[0]\n",
    "print(mode_value)\n",
    "\n",
    "# Fill nulls with the mode\n",
    "df2['studio'] = df2['studio'].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only numeric columns with missing values\n",
    "missing_num_col = df2.select_dtypes('number').isna().sum()\n",
    "print(missing_num_col,'\\n')\n",
    "\n",
    "# checking the skewness of the gross amounts to determine how best to deal with the missing data\n",
    "print(\"Skewness of domestic_gross:\", df2['domestic_gross'].skew())\n",
    "print(\"Skewness of foreign_gross:\", df2['foreign_gross'].skew())\n",
    "\n",
    "# both are highly right-skewed. \n",
    "# Since the skew is >1, using the mean would be misleading because large outliers inflate it.\n",
    "# and using the median would only be okay if the msissing values was actually an error,not an absence of data\n",
    "# to avoid distorting the values by extreme outliers\n",
    "# but in this case, we will make a compromise to assume that the movies did not have a foreign or domestic revenue\n",
    "# therefore filling with 0 (zero) instead of the median\n",
    "\n",
    "# Fill missing values using the median\n",
    "df2['domestic_gross'] = df2['domestic_gross'].fillna(0)\n",
    "df2['foreign_gross'] = df2['foreign_gross'].fillna(0)\n",
    "\n",
    "# creating a new column total_gross \n",
    "df2['total_gross'] = df2['domestic_gross'] + df2['foreign_gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm that we do not have any missing values, numbers and objects:\n",
    "missing_num_col = df2.select_dtypes('number').isna().sum()\n",
    "print(missing_num_col,'\\n')\n",
    "\n",
    "missing_obj_col = df2.select_dtypes(include='object').isna().sum()\n",
    "print(missing_obj_col,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the columns we will be working on:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to check for skewness and outliers\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df2['domestic_gross'])\n",
    "plt.title('Boxplot of Domestic Gross')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df2['foreign_gross'])\n",
    "plt.title('Boxplot of Foreign Gross')\n",
    "plt.show()\n",
    "# box the domestic and foreign gross are heavily right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the DataFrame to CSV\n",
    "df2.to_csv('../Cleaned_Data/cleaned_bom.movie_gross.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
